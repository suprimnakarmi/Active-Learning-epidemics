{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required python libraries         \n",
    "import numpy as np         \n",
    "import os                  \n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# OpenCV and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2 \n",
    "\n",
    "# Pandas \n",
    "# import pandas as pd\n",
    "\n",
    "# Tensorflow\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "# from tensorflow.keras.applications.densenet import DenseNet169\n",
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# from tensorflow.keras.applications.resnet import ResNet101 \n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model = int(input(\"Enter the number for: \\n 1) VGGNET16 \\n 2) Resnet101  \\n 3) Densenet161 \"))\n",
    "\n",
    "select_distance = int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./x-ray_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0,1] # 1 = Covid // 0 = Noncovid \n",
    "\n",
    "all_files=[]\n",
    "\n",
    "for i in os.listdir(dataset_path):  # Get all the files from the directory in a two element list. First element is list of file location to covid images and second element is list of file location to non-covid images.\n",
    "  file1 = glob.glob(os.path.join(dataset_path,i, \"*.png\"))\n",
    "  file2 = glob.glob(os.path.join(dataset_path,i, \"*.jpg\")) # .jpg files are also present.\n",
    "  file1.extend(file2)  # Only extends when there is .jpg file present\n",
    "  all_files.append(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0     # Count to record the ids of files. Each file has a unique ID.\n",
    "img_size = 224\n",
    "def get_dataset(files, label,count):        \n",
    "  dataset=[]  # List to hold all the dataset. Each element is a dictionary\n",
    "  \n",
    "  for j in tqdm(files):  # Loop over each file location\n",
    "    data_dict = {}  \n",
    "    data_dict[\"id\"] = count\n",
    "    data_dict[\"filepath\"] = j\n",
    "    img=cv2.imread(j)\n",
    "    img = cv2.resize(img,(img_size,img_size))\n",
    "    data_dict[\"image\"]= img\n",
    "    data_dict[\"label\"]= label\n",
    "    count +=1\n",
    "    dataset.append(data_dict)\n",
    "  return dataset, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dataset, nc_dataset, t_dataset = [], [], []  \n",
    "\n",
    "for i,data in enumerate(all_files[1:]):\n",
    "  dataset,count=get_dataset(data,labels[i],count)\n",
    "  if labels[i]==1:\n",
    "    c_dataset = dataset\n",
    "  else:\n",
    "    nc_dataset = dataset\n",
    "t_dataset = c_dataset + nc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4715\n"
     ]
    }
   ],
   "source": [
    "print(len(t_dataset))\n",
    "batch_size=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only, label_only, id_only, img_name = [], [], [], []\n",
    "for data in t_dataset:\n",
    "  image_only.append(data[\"image\"])\n",
    "  label_only.append(data[\"label\"]) \n",
    "  id_only.append(data['id'])\n",
    "  img_name.append(data[\"filepath\"].split(\"/\")[-1])\n",
    "image_only=np.array(image_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = ImageDataGenerator()\n",
    "batch_img= img_datagen.flow(image_only, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models(img_size, model_sel):\n",
    " \n",
    "  if model_sel == 1:\n",
    "    vgg_pre_t = VGG16(input_shape = (img_size, img_size, 3),include_top = False, weights ='imagenet')\n",
    "    return vgg_pre_t, 25088\n",
    "\n",
    "  elif model_sel==2:\n",
    "    resnet_pre_t= ResNet101(input_shape = (img_size, img_size, 3),include_top=False, weights='imagenet')\n",
    "    return resnet_pre_t, 100352\n",
    "\n",
    "  elif model_sel==3:\n",
    "    densenet169_pre_t = DenseNet169(input_shape = (img_size, img_size, 3),include_top = False, weights ='imagenet' )\n",
    "    return densenet169_pre_t, 81536\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fea = []\n",
    "model,feature_size= all_models(img_size, select_model)\n",
    "for data in tqdm(range(len(batch_img))):\n",
    "  try:\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(batch_size,feature_size)\n",
    "  except:\n",
    "    img_len=len(batch_img[data])\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(img_len,feature_size)\n",
    "  all_fea.extend(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(t_dataset)):\n",
    "  t_dataset[i]['image']= all_fea[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_files/image_net/VGGNet16.pickle','rb') as handle:\n",
    "   t_dataset  = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4715"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 283,\n",
       " 'filepath': '/content/gdrive/My Drive/Dataset/Covid/MIDRC-RICORD-1C-419639-003112-48985-0.png',\n",
       " 'image_features': array([ 0.       ,  5.3459306,  2.563743 , ...,  0.       , 18.554409 ,\n",
       "         0.       ], dtype=float32),\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "shuffle(t_dataset)\n",
    "t_dataset.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_clusters(features):\n",
    "    kmeans = KMeans(n_clusters=8, random_state=0, n_init=\"auto\").fit(features)\n",
    "    output= kmeans.labels_\n",
    "    clusters = [np.squeeze(np.array(features)[[np.where(output==i)[0]]],axis=0) for i in range(len(np.unique(output)))]\n",
    "    return kmeans.cluster_centers_, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separation(dataset,label):\n",
    "    add_data= []\n",
    "    i=0\n",
    "    while len(add_data)!=20:\n",
    "        if dataset[i][\"label\"]==label:\n",
    "            add_data.append(dataset[i]['image_features'])\n",
    "            del dataset[i]\n",
    "        i+=1\n",
    "    return add_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_features(positive, negative):\n",
    "    # print(f\"pure_pf: {positive}\")\n",
    "    # print(f\"p_type: {type(positive)}\")\n",
    "    # print(f\"len_p: {len(positive)}\")\n",
    "    mpos_features=np.array([np.mean(i,axis=0) for i in positive])  # Mean of all positive sub clusters \n",
    "    mneg_features=np.array([np.mean(i,axis=0) for i in negative])  # Mean of all negative sub clusters\n",
    "    # print(mpos_features)\n",
    "    return mpos_features, mneg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[223], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m a\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray([[[\u001b[39m2\u001b[39m],[\u001b[39m5\u001b[39m],[\u001b[39m22\u001b[39m],[\u001b[39m90\u001b[39m]], \n\u001b[1;32m      2\u001b[0m             [[\u001b[39m52\u001b[39m],[\u001b[39m28\u001b[39m],[\u001b[39m1\u001b[39m]],\n\u001b[1;32m      3\u001b[0m             [[\u001b[39m44\u001b[39m],[\u001b[39m28\u001b[39m,\u001b[39m27\u001b[39m],[\u001b[39m54\u001b[39m],[\u001b[39m97\u001b[39m],[\u001b[39m89\u001b[39m]],\n\u001b[1;32m      4\u001b[0m             [[\u001b[39m42\u001b[39m],[\u001b[39m35\u001b[39m],[\u001b[39m82\u001b[39m]]],dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# np.argmax(np.array(a))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# b=np.concatenate((a[-1],np.array([[69]])),axis=0)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# b = np.append(a[0],np.array([[99,20,0,9]]),axis=0)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# z=list(a)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# z.append(np.array([[43,67,86]]))\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m a[\u001b[39m0\u001b[39m]\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mconcatenate((a[\u001b[39m0\u001b[39;49m],[\u001b[39m71068\u001b[39;49m]),axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     14\u001b[0m a\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "a=np.array([[[2],[5],[22],[90]], \n",
    "            [[52],[28],[1]],\n",
    "            [[44],[28,27],[54],[97],[89]],\n",
    "            [[42],[35],[82]]],dtype=object)\n",
    "# np.argmax(np.array(a))\n",
    "# b=np.concatenate((a[-1],np.array([[69]])),axis=0)\n",
    "# b = np.append(a[0],np.array([[99,20,0,9]]),axis=0)\n",
    "# b=np.array([99,20,0,9])\n",
    "# np.insert(a,np.array([[99,20,0,9]]))\n",
    "# c=np.stack((a,b))\n",
    "# z=list(a)\n",
    "# z.append(np.array([[43,67,86]]))\n",
    "a[0]=np.concatenate((a[0],[[71068]]),axis=0)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_subclusters(all_dist, query, fea_label, id_pred, label_pred, features, decision, n_neighbours, cluster):\n",
    "    max_ind=np.argmax(all_dist)\n",
    "    features[max_ind]=np.concatenate((features[max_ind],np.expand_dims(query[\"image_features\"], axis=0)),axis=0)\n",
    "    # fea_label[cluster].append(np.expand_dims(query[\"image_features\"], axis=0))  # Have doubt here\n",
    "    id_pred[cluster].append(query[\"id\"])\n",
    "    label_pred[cluster].append((query['id'],decision.count(1)/n_neighbours))\n",
    "    return features, fea_label, id_pred, label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mispredictions(query, fea_label,train_label, train_id, ind_data, decision,data_frame_1, count, pos_dist, neg_dist, pos_features, neg_features):\n",
    "    if mode(decision) != query[\"label\"]:\n",
    "        # print(\"here\")\n",
    "        count +=1 \n",
    "        data_frame_1[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])\n",
    "        data_frame_1[\"Mistake ID\"].append(query['id'])\n",
    "        data_frame_1[\"Original label\"].append(query['label'])\n",
    "        data_frame_1[\"Predicted label\"].append(mode(decision))\n",
    "        data_frame_1[\"Mistake index\"].append(ind_data)\n",
    "        if query[\"label\"]==1:\n",
    "            pos_features_list= list(pos_features)\n",
    "            pos_features_list.append(np.expand_dims(query[\"image_features\"], axis=0))\n",
    "            pos_features = np.array(pos_features_list)\n",
    "            # pos_features= np.concatenate((pos_features,np.expand_dims(query[\"image_features\"], axis=0)),axis=0)\n",
    "        else:\n",
    "            neg_features_list= list(neg_features)\n",
    "            neg_features_list.append(np.expand_dims(query[\"image_features\"], axis=0))\n",
    "            neg_features = np.array(neg_features_list)\n",
    "            # neg_features = np.concatenate((neg_features,np.expand_dims(query[\"image_features\"], axis=0)),axis=0)\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "\n",
    "    else:\n",
    "        if query['label'] == 0:\n",
    "            max_ind = np.argmax(neg_dist)\n",
    "            neg_features[max_ind] = np.concatenate((neg_features[max_ind],np.expand_dims(query[\"image_features\"], axis=0)),axis=0)\n",
    "            # fea_label[query['label']].append(np.concatenate((fea_label[query['label']],np.expand_dims(query[\"image_features\"], axis=0)),axis=0))\n",
    "        else:\n",
    "            max_ind = np.argmax(pos_dist)\n",
    "\n",
    "            pos_features[max_ind] = np.concatenate((pos_features[max_ind],np.expand_dims(query[\"image_features\"], axis=0)),axis=0)\n",
    "            # fea_label[query['label']].append(np.concatenate((mpos_features,np.expand_dims(query[\"image_features\"], axis=0)),axis=0))\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    return count,data_frame_1,fea_label,train_label,train_id,pos_features,neg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance2(query, fea_label, select_distance, id_pred, label_pred, n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features, supervised_data): # Query is the raw dictionary (from pickle file) // fea_label is dictionary of {0: [], 1:[]} (distance) // select distance is int\n",
    "  exp_query = np.expand_dims(query['image_features'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "\n",
    "  if select_distance==1: # Euclidean distance\n",
    "    # print(f\"Type: {type(fea_label[0])}\")\n",
    "    # print(f\"Shape: {fea_label[0].shape}\")\n",
    "    neg_dist = np.linalg.norm(query['image_features']- fea_label[0], axis=1)  # Calculating the Euclidean distance using numpy (axis=1) to calculate all at ones   \n",
    "    pos_dist = np.linalg.norm(query['image_features']- fea_label[1], axis=1)\n",
    "\n",
    "  # elif select_distance==2: # Manhattan distance\n",
    "  #   neg_dist = np.squeeze(manhattan_distances(fea_label[0],exp_query))  # convert (1,n) to (,n)\n",
    "  #   pos_dist=np.squeeze(manhattan_distances(fea_label[1],exp_query))\n",
    "\n",
    "  # elif select_distance==3: # Cosine distance\n",
    "  #   neg_dist = np.squeeze(cosine_distances(exp_query,fea_label[0]))  # convert (1,n) to (,n)\n",
    "  #   pos_dist=np.squeeze(cosine_distances(exp_query,fea_label[1]))\n",
    "  \n",
    "  for dist_single in pos_dist:\n",
    "    # print(dist_single)\n",
    "    pos_tup.append((dist_single,1))\n",
    "\n",
    "  for dist_single in neg_dist:\n",
    "    neg_tup.append((dist_single,0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)\n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]\n",
    "  \n",
    "  decision = [y for (x,y) in tup_dist]\n",
    "\n",
    "  if supervised_data:\n",
    "    count,data_frame_1,fea_label,train_label,train_id, pos_features,neg_features=correct_mispredictions(query, fea_label,train_label,train_id, ind_data, decision,data_frame_1, count, pos_dist, neg_dist, pos_features, neg_features)\n",
    "    \n",
    "  else:\n",
    "    if decision.count(0) > decision.count(1):\n",
    "      neg_features, fea_label, id_pred, label_pred = update_subclusters(neg_dist,query,fea_label,id_pred,label_pred,neg_features, decision, n_neighbours, cluster=0)\n",
    "      \n",
    "    else:\n",
    "      pos_features, fea_label, id_pred, label_pred = update_subclusters(pos_dist,query,fea_label,id_pred,label_pred,pos_features, decision,n_neighbours, cluster=1)\n",
    "  \n",
    "  return id_pred, label_pred, data_frame_1, count, train_label, train_id, pos_features, neg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(label_gt,id_pred):\n",
    "  TP,FP,FN,TN = 0,0,0,0\n",
    "\n",
    "  for tp in id_pred[1]:   # TP --> When correctly classified covid\n",
    "    if tp in label_gt[1]:\n",
    "      TP +=1\n",
    "\n",
    "  for tn in id_pred[0]:  # TN --> When correctly classified healthy (non-covid)\n",
    "    if tn in label_gt[0]:\n",
    "      TN +=1\n",
    "\n",
    "  for fp in id_pred[1]: # FP --> When incorrectly classified healthy (Classified healthy as covid)\n",
    "    if fp in label_gt[0]:\n",
    "      FP +=1\n",
    "\n",
    "  for fn in id_pred[0]: # FN --> When missed covid classification (Covid cases missed)\n",
    "    if fn in label_gt[1]:\n",
    "      FN +=1\n",
    "\n",
    "  accuracy= (TP+TN)/(TP+TN+FP+FN)\n",
    "  specificity = TN/(TN+FP)\n",
    "  sensitivity = (TP)/(TP+FN)\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "  \n",
    "  print(\"TP: \", TP)\n",
    "  print(\"FP: \", FP)\n",
    "  print(\"FN: \", FN)\n",
    "  print(\"TN: \", TN)\n",
    "\n",
    "  return accuracy, specificity, sensitivity,TP,TN,FP,FN\n",
    "\n",
    "def roc_auc_curve(label_gt,label_pred):\n",
    "  gt_labels= sorted(label_gt[0]+ label_gt[1])  # Contains (id,labels) tuple of binary class \n",
    "  pred_labels = sorted(label_pred[0]+label_pred[1]) # Contains (id,labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  y_test = [y for (x,y) in gt_labels]   # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  return roc_auc\n",
    "\n",
    "def cluster_metrics(pos_features, neg_features, train_label,id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  intra_dist1 = euclidean_distances(neg_features).max()\n",
    "  intra_dist2 = euclidean_distances(pos_features).max()\n",
    "  inter_dist = euclidean_distances(neg_features,pos_features).min()\n",
    "\n",
    "  if intra_dist1>intra_dist2:\n",
    "    max_intra_dist= intra_dist1  \n",
    "  else:\n",
    "    max_intra_dist = intra_dist2 \n",
    "\n",
    "  Dunn_index = inter_dist/max_intra_dist\n",
    "\n",
    "  print(\"Calculating Davies Bouldin index...\")\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  class_0 =np.concatenate((np.zeros(shape=(len(train_label[0])),dtype=int),np.zeros(shape=(len(id_pred[0])),dtype=int),np.zeros(shape=(20),dtype=int)))\n",
    "  class_1 = np.concatenate((np.ones(shape=(len(train_label[1])),dtype=int),np.ones(shape=(len(id_pred[1])),dtype=int),np.zeros(shape=(20),dtype=int)))\n",
    "  class_all = np.concatenate((class_0,class_1))\n",
    "  feature_all = np.concatenate((neg_features,pos_features))\n",
    "\n",
    "  davies_bouldin_index = davies_bouldin_score(feature_all,class_all)\n",
    "  silhouette_index = silhouette_score(feature_all,class_all)\n",
    "\n",
    "  print(\"davies: \", davies_bouldin_index)\n",
    "  print(\"silhouette_sklearn: \", silhouette_index)\n",
    "  \n",
    "  return Dunn_index,davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_size = [200,400,800,1100,1300,1550]\n",
    "labeled_size = [1550]\n",
    "def data_loader(dataset,n): # Method to return three sets of labeled dataset for experiment\n",
    "  labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "  l_data = dataset[:n]    # First dataset // labeled\n",
    "  ul_data = dataset[n:]   # First dataset // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[1500:1500+n]    # second dataset // labeled\n",
    "  ul_data = dataset[:1500]+dataset[1500+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[3000:3000+n]     # Third dataset // labeled\n",
    "  ul_data = dataset[:3000]+dataset[3000+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_features(features):\n",
    "    all_features = []\n",
    "    for i in features:\n",
    "        for j in i:\n",
    "            all_features.append(j)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/yxqczwgn1dnd8lbd0_cn2tkw0000gn/T/ipykernel_23375/121668272.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pos_features = np.array(pos_features_list)\n",
      "/var/folders/vf/yxqczwgn1dnd8lbd0_cn2tkw0000gn/T/ipykernel_23375/121668272.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  neg_features = np.array(neg_features_list)\n",
      "100%|██████████| 3164/3164 [00:50<00:00, 62.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  1573\n",
      "FP:  190\n",
      "FN:  8\n",
      "TN:  1393\n",
      "Calculating Dunn's index...\n",
      "Calculating Davies Bouldin index...\n",
      "davies:  2.716889398994854\n",
      "silhouette_sklearn:  0.11548696\n",
      "Labeled image: 1550 \t Dataset: d_0 \t Accuracy: 0.9374209860935525 \t Specificity: 0.8799747315224258 \t Sensitivity: 0.9949399114484504 \t Dunn index: 1.6634847952445853e-06  \t Davies Bouldin: 2.716889398994854 \t Silhouette index: 0.11548695713281631 \t AUC: 0.9450981990416039 \t Corrected count: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/yxqczwgn1dnd8lbd0_cn2tkw0000gn/T/ipykernel_23375/121668272.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  neg_features = np.array(neg_features_list)\n",
      "/var/folders/vf/yxqczwgn1dnd8lbd0_cn2tkw0000gn/T/ipykernel_23375/121668272.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  pos_features = np.array(pos_features_list)\n",
      " 24%|██▎       | 750/3164 [00:07<00:25, 94.83it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m   fea_label\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: mneg_features,\n\u001b[1;32m    114\u001b[0m         \u001b[39m1\u001b[39m: mpos_features}\n\u001b[1;32m    116\u001b[0m   id_pred, label_pred, _, _, _, _, pos_features, neg_features \u001b[39m=\u001b[39m distance2(data,fea_label,\u001b[39m1\u001b[39m,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features,supervised_data\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m# ind_data is the index of misclassification\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m   mpos_features, mneg_features \u001b[39m=\u001b[39m mean_features(pos_features, neg_features)    \u001b[39m# Get the mean of the features\u001b[39;00m\n\u001b[1;32m    119\u001b[0m accuracy, specificity, sensitivity,TP,TN,FP,FN\u001b[39m=\u001b[39m classification_metrics(id_gt,id_pred)\n\u001b[1;32m    120\u001b[0m flattened_pos_features \u001b[39m=\u001b[39m flatten_features(pos_features) \n",
      "Cell \u001b[0;32mIn[147], line 6\u001b[0m, in \u001b[0;36mmean_features\u001b[0;34m(positive, negative)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_features\u001b[39m(positive, negative):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# print(f\"pure_pf: {positive}\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m# print(f\"p_type: {type(positive)}\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m# print(f\"len_p: {len(positive)}\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     mpos_features\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mmean(i,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m positive])  \u001b[39m# Mean of all positive sub clusters \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     mneg_features\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49marray([np\u001b[39m.\u001b[39;49mmean(i,axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m negative])  \u001b[39m# Mean of all negative sub clusters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m# print(mpos_features)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m mpos_features, mneg_features\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_neighbours=15\n",
    "\n",
    "data_frame = {\"Labeled data\": [],\n",
    "              \"Dataset\": [],\n",
    "              \"Accuracy\": [],\n",
    "              \"Specificity\": [],\n",
    "              \"Sensitivity\": [],\n",
    "              \"AUC\":[],\n",
    "              \"Dunn index\": [],\n",
    "              \"Davies Bouldin\": [],\n",
    "              \"Silhouette index\":[],\n",
    "              \"TP\":[],\n",
    "              \"TN\":[],\n",
    "              \"FP\":[],\n",
    "              \"FN\":[],\n",
    "              \"pos_labeled_img\":[],\n",
    "              \"neg_labeled_img\":[],\n",
    "              \"corrected_count\":[]\n",
    "    \n",
    "}\n",
    "# fea_label1={0: [],\n",
    "#             1:[]}\n",
    "\n",
    "\n",
    "for size in labeled_size:\n",
    "  labeled_data, unlabeled_data = data_loader(t_dataset, size)\n",
    "#   print(f\"labeled data length {len(labeled_data)}\")\n",
    "#   print(f\"Unlabeled data length {len(unlabeled_data)}\")\n",
    "  select=0         # To select the dataset out of three sets ==> three sets: [d11, d12, d13] ==> eg: [200,200,200]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  while(select < 3):\n",
    "    data_frame_1 = {  \"Image name\": [],\n",
    "                  \"Mistake index\": [],\n",
    "                  \"Mistake ID\": [],\n",
    "                  \"Original label\": [],\n",
    "                  \"Predicted label\": []\n",
    "                  \n",
    "    }\n",
    "    pos_img, neg_img=0, 0\n",
    "\n",
    "    fpos, fneg= [], []\n",
    "\n",
    "    label_gt = {0: [],    \n",
    "        1 :[]}    \n",
    "                            # Collect the ground truth (label) of all the predicting images\n",
    "    train_label = {0: [],    \n",
    "        1 :[]}    \n",
    "\n",
    "    label_pred = {0: [],\n",
    "        1 :[]}               # Collect the predicted label for all the images\n",
    "\n",
    "    id_gt = {0: [], \n",
    "            1: [] }         # Collect the ground truth (id) of all the predicting images\n",
    "\n",
    "    id_pred = {0: [],\n",
    "            1: []}        # Collect the predicted id for all the images \n",
    "\n",
    "    fea_label = {0: [],\n",
    "            1: []}\n",
    "\n",
    "    train_id ={0: [],\n",
    "            1:[]}\n",
    "        \n",
    "    # print(type(labeled_data[0][0]))\n",
    "    # for data in labeled_data[select]:\n",
    "    #     if data[\"label\"] == 1:\n",
    "    #         fpos.append(data['image_features'])\n",
    "    #         train_id[1].append(data['id'])\n",
    "    #         train_label[1].append((data['id'],data['label']))\n",
    "    #         pos_img +=1\n",
    "\n",
    "    #     else:\n",
    "    #         fneg.append(data['image_features'])\n",
    "    #         train_id[0].append(data['id'])\n",
    "    #         train_label[0].append((data['id'],data['label']))\n",
    "    #         neg_img +=1\n",
    "\n",
    "    # print(f\"Blen: {len(labeled_data[select])}\")\n",
    "    fpositive = data_separation(labeled_data[select],1)    # Get 20 features of each class\n",
    "\n",
    "    \n",
    "    fnegative = data_separation(labeled_data[select],0)\n",
    "\n",
    "\n",
    "    mneg_features,neg_features= sub_clusters(fnegative)  # Get the subclusters (Using K-means algorithm)\n",
    "    mpos_features,pos_features= sub_clusters(fpositive)    \n",
    "\n",
    "        \n",
    "\n",
    "    count, ind_data=0, 40\n",
    "    for data in labeled_data[select]:\n",
    "        fea_label={0: mneg_features,\n",
    "            1: mpos_features}\n",
    "        id_pred, label_pred, data_frame_1, count, train_label, train_id, pos_features, neg_features= distance2(data,fea_label,1,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features, supervised_data=True)\n",
    "        mpos_features, mneg_features = mean_features(pos_features, neg_features)    # Get the mean of the features\n",
    "        ind_data +=1\n",
    "\n",
    "    data_f_1 = pd.DataFrame.from_dict(data_frame_1)\n",
    "    data_f_1.to_csv(f\"./csv_results_x-ray_counts/new/resnet101_euclidean_mistake_{size}_{select}.csv\",index=False)\n",
    "\n",
    "    for data in tqdm(unlabeled_data[select]):\n",
    "      if data[\"label\"]==1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'],data['label']))\n",
    "      \n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'],data['label']))\n",
    "      \n",
    "      fea_label={0: mneg_features,\n",
    "            1: mpos_features}\n",
    "\n",
    "      id_pred, label_pred, _, _, _, _, pos_features, neg_features = distance2(data,fea_label,1,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1, pos_features, neg_features,supervised_data=False) # ind_data is the index of misclassification\n",
    "      mpos_features, mneg_features = mean_features(pos_features, neg_features)    # Get the mean of the features\n",
    "\n",
    "    accuracy, specificity, sensitivity,TP,TN,FP,FN= classification_metrics(id_gt,id_pred)\n",
    "    flattened_pos_features = flatten_features(pos_features) \n",
    "    flattened_neg_features = flatten_features(neg_features)\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrics(flattened_pos_features, flattened_neg_features, train_label,id_pred)\n",
    "    cl_auc = roc_auc_curve(label_gt,label_pred)\n",
    "    data_frame[\"Labeled data\"].append(size)\n",
    "    data_frame[\"Dataset\"].append(f\"d_{select}\")\n",
    "    data_frame[\"Accuracy\"].append(accuracy)\n",
    "    data_frame[\"Specificity\"].append(specificity)\n",
    "    data_frame[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame[\"AUC\"].append(cl_auc)\n",
    "    data_frame[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame[\"TP\"].append(TP)\n",
    "    data_frame[\"TN\"].append(TN)\n",
    "    data_frame[\"FP\"].append(FP)\n",
    "    data_frame[\"FN\"].append(FN)\n",
    "    data_frame[\"pos_labeled_img\"].append(pos_img)\n",
    "    data_frame[\"neg_labeled_img\"].append(neg_img)\n",
    "    data_frame[\"corrected_count\"].append(count)\n",
    "\n",
    "    print(f\"Labeled image: {size} \\t Dataset: d_{select} \\t Accuracy: {accuracy} \\t Specificity: {specificity} \\t Sensitivity: {sensitivity} \\t Dunn index: {dunn_index}  \\t Davies Bouldin: {davies_bouldin_index} \\t Silhouette index: {silhouette_index} \\t AUC: {cl_auc} \\t Corrected count: {count}\")\n",
    "    select +=1 \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mpos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model, select_distance = 2,2\n",
    "if select_model==1:\n",
    "    s_model= 'vgg16'\n",
    "elif select_model==2:\n",
    "    s_model= 'resnet101'\n",
    "elif select_model==3:\n",
    "    s_model='densenet169'\n",
    "\n",
    "if select_distance==1:\n",
    "    s_distance='euclidean'\n",
    "elif select_distance==2:\n",
    "    s_distance='manhattan'\n",
    "elif select_distance==3:\n",
    "    s_distance='cosine'\n",
    "data_f=pd.DataFrame.from_dict(data_frame)\n",
    "data_f.to_csv(f\"./csv_results_x-ray_counts/{s_model}_{s_distance}_dist_1100_1300.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyz(a,*args, **kwargs):\n",
    "    print(a)\n",
    "    print(kwargs.get('dist'))\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "None\n",
      "([1, 2, 3],)\n"
     ]
    }
   ],
   "source": [
    "xyz(1,[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"dist\":123,\n",
    "          \"ok\":12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "123\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "xyz(1, **config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
